{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05674093  0.06082482]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Define the dtype\n",
    "dtype = jnp.float32\n",
    "\n",
    "# Define the target state\n",
    "target_state = jnp.array([0.0, 0.0], dtype=dtype)\n",
    "\n",
    "# Define the noise level\n",
    "noise_level = 0.1\n",
    "target_state = jnp.array([0.0, 0.0], dtype=dtype)\n",
    "\n",
    "# Generate random noise\n",
    "key = jax.random.PRNGKey(0)\n",
    "noise = jax.random.uniform(key, shape=target_state.shape, minval=-noise_level, maxval=noise_level, dtype=dtype)\n",
    "\n",
    "# Add noise to the target state\n",
    "target_state_with_noise = target_state + noise\n",
    "\n",
    "print(target_state_with_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_state = jnp.array([0.0, 0.0], dtype=dtype)\n",
    "state = jnp.array([0.5, 0.4], dtype=dtype)\n",
    "is_goal = jnp.all(jnp.abs(state - target_state) <= 1)\n",
    "is_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit\n",
    "def example_reward_function(state, goal_state):\n",
    "    \"\"\"Define your reward logic here.\"\"\"\n",
    "    # if jnp.array_equal(state, jnp.array([4, 4])):\n",
    "    #    return 10\n",
    "    # else:\n",
    "    #    return -1\n",
    "    is_goal = jnp.all(state == goal_state)\n",
    "    current_distance = jnp.linalg.norm(state - goal_state)\n",
    "    total_distance = jnp.sqrt(2)\n",
    "    \n",
    "    distance_reward = current_distance/total_distance * 3\n",
    "    \n",
    "    # Use jnp.where to select between goal reward and distance-based reward\n",
    "    reward = jnp.where(is_goal, 15.0, distance_reward)\n",
    "    return reward\n",
    "\n",
    "def example_transition_function(state, action, state_space_shape):\n",
    "    \"\"\"Define your state transition logic here.\"\"\"\n",
    "    x, y = state\n",
    "\n",
    "    def magic(key):\n",
    "        return random.uniform(random.PRNGKey(key), (1,2), minval=-1, maxval=1)[0]\n",
    "\n",
    "    def action_one(_):\n",
    "        return x+y\n",
    "\n",
    "    def action_two(_):\n",
    "        return jnp.abs(x-y)\n",
    "\n",
    "    def action_three(_):\n",
    "        return x/y\n",
    "\n",
    "    def action_four(_):\n",
    "        return y/x\n",
    "\n",
    "    key = jax.lax.switch(action, [action_one,action_two,action_three,action_four], None)\n",
    "    x, y = magic(key) * key\n",
    "    return jnp.array([x, y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import jax\n",
    "from jax import jit, random, vmap, numpy as jnp\n",
    "\n",
    "import optax\n",
    "\n",
    "import random as rd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"episode: {episode} state: {state}, reward: {reward}, action: none done: {done}\")\n",
    "#episode_reward = 0  # Track total reward per episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = rd.sample(agent.memory, batch_size)\n",
    "states = jnp.array([experience[0] for experience in minibatch], dtype=jnp.float32)\n",
    "actions = jnp.array([experience[1] for experience in minibatch], dtype=jnp.int32)\n",
    "rewards = jnp.array([experience[2] for experience in minibatch], dtype=jnp.float32)\n",
    "next_states = jnp.array([experience[3] for experience in minibatch], dtype=jnp.float32)\n",
    "dones = jnp.array([experience[4] for experience in minibatch], dtype=jnp.bool_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the target Q-values using JIT compilation\n",
    "@jax.jit\n",
    "def compute_target_q_values(rewards, gamma, futures, dones):\n",
    "    return rewards + gamma * futures * (1 - dones) \n",
    "\n",
    "gamma = agent.gamma\n",
    "futures = jnp.max(agent.model.apply(agent.state.params, next_states), axis=-1)\n",
    "target_q_values = compute_target_q_values(rewards, gamma, futures, dones)\n",
    "type(target_q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(states, actions, targets, model, params):\n",
    "    \"\"\"\n",
    "    Compute the loss values in parallel using JAX.\n",
    "\n",
    "    :param states: Array of states\n",
    "    :param actions: Array of actions\n",
    "    :param targets: Array of target Q-values\n",
    "    :param model: A Flax model\n",
    "    :param params: Parameters of the Flax model\n",
    "    :return: Array of loss values\n",
    "    \"\"\"\n",
    "    # Vectorize the model function to apply it to all states\n",
    "    def model_fn(state):\n",
    "        return model.apply(params, state)\n",
    "    \n",
    "    vectorized_model = vmap(model_fn)\n",
    "    \n",
    "    # Compute the Q-values for all states\n",
    "    q_values = vectorized_model(states)\n",
    "    \n",
    "    # Select the Q-values corresponding to the actions taken\n",
    "    q_values = jnp.take_along_axis(q_values, actions[:, None], axis=1).squeeze()\n",
    "    \n",
    "    # Compute the squared differences between predicted Q-values and target Q-values\n",
    "    loss_values = (q_values - targets) ** 2\n",
    "    \n",
    "    return loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = compute_loss(states, actions, target_q_values, agent.model, agent.state.params)\n",
    "G = loss_fn_batch(states, actions, target_q_values, agent.model, agent.state.params)\n",
    "L == G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 3: Average all the gradients then just apply one\n",
    "# In this approach, we compute the gradients for all samples, average them, and then apply the averaged gradient.\n",
    "def loss_fn_batch(states, actions, targets, model, params):\n",
    "    def single_loss_fn(state, action, target):\n",
    "        q_values = model.apply(params, state)\n",
    "        q_value = q_values[action]\n",
    "        loss = (q_value - target) ** 2\n",
    "        return loss\n",
    "    vectorized_loss_fn = jax.vmap(single_loss_fn, in_axes=(0, 0, 0))\n",
    "    return vectorized_loss_fn(states, actions, targets)\n",
    "\n",
    "\n",
    "def update_step_average(states, actions, targets, model, train_state):\n",
    "    grads = jax.grad(loss_fn_batch)(train_state.params, states, actions, targets, model)\n",
    "    mean_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "    train_state = train_state.apply_gradients(grads=mean_grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "#train_state = update_step_average(states, actions, targets, model, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, state, action, target):\n",
    "    q_values = agent.model.apply(params, state)\n",
    "    q_value = q_values[action]\n",
    "    loss = jnp.mean((target - q_value) ** 2)\n",
    "    return loss\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "vmap_grad_fn = vmap(grad_fn, in_axes=(None, 0, 0, 0))\n",
    "\n",
    "sgrads = grad_fn(agent.state.params, states[8], actions[8], target_q_values[8])\n",
    "grads = vmap_grad_fn(agent.state.params, states, actions, target_q_values)\n",
    "average_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 2: Calculate all the gradients and then apply them all\n",
    "#In this approach, we compute the gradients for all samples and then apply them all at once.\n",
    " \n",
    "def loss_fn_batch(params, states, actions, targets, model):\n",
    "    def single_loss_fn(state, action, target):\n",
    "        q_values = model.apply(params, state)\n",
    "        q_value = q_values[action]\n",
    "        loss = (q_value - target) ** 2\n",
    "        return loss\n",
    "    vectorized_loss_fn = jax.vmap(single_loss_fn, in_axes=(None, 0, 0, 0))\n",
    "    return jnp.mean(vectorized_loss_fn(states, actions, targets))\n",
    "\n",
    "def update_step_batch(states, actions, targets, model, train_state):\n",
    "    grads = jax.grad(loss_fn_batch)(train_state.params, states, actions, targets, model)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "train_state = update_step_batch(states, actions, targets, model, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Get gradient, apply it, get the next one, apply it, etc.\n",
    "# In this approach, we compute and apply the gradient for each sample sequentially.\n",
    "\n",
    "import jax\n",
    "from flax.training import train_state\n",
    "\n",
    "def loss_fn(params, state, action, target, model):\n",
    "    q_values = model.apply(params, state)\n",
    "    q_value = q_values[action]\n",
    "    loss = (q_value - target) ** 2\n",
    "    return loss\n",
    "\n",
    "def update_step(state, action, target, model, train_state):\n",
    "    grads = jax.grad(loss_fn)(train_state.params, state, action, target, model)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "for state, action, target in zip(states, actions, targets):\n",
    "    train_state = update_step(state, action, target, model, train_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
