{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gymnasium import spaces, Env\n",
    "import jax\n",
    "from jax import jit, numpy as jnp\n",
    "from jax import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit\n",
    "def example_reward_function(state, goal_state):\n",
    "    \"\"\"Define your reward logic here.\"\"\"\n",
    "    # if jnp.array_equal(state, jnp.array([4, 4])):\n",
    "    #    return 10\n",
    "    # else:\n",
    "    #    return -1\n",
    "    is_goal = jnp.all(state == goal_state)\n",
    "    current_distance = jnp.linalg.norm(state - goal_state)\n",
    "    total_distance = jnp.sqrt(2)\n",
    "    \n",
    "    distance_reward = current_distance/total_distance * 3\n",
    "    \n",
    "    # Use jnp.where to select between goal reward and distance-based reward\n",
    "    reward = jnp.where(is_goal, 15.0, distance_reward)\n",
    "    return reward\n",
    "\n",
    "def example_transition_function(state, action, state_space_shape):\n",
    "    \"\"\"Define your state transition logic here.\"\"\"\n",
    "    x, y = state\n",
    "\n",
    "    def magic(key):\n",
    "        return random.uniform(random.PRNGKey(key), (1,2), minval=-1, maxval=1)[0]\n",
    "\n",
    "    def action_one(_):\n",
    "        return x+y\n",
    "\n",
    "    def action_two(_):\n",
    "        return jnp.abs(x-y)\n",
    "\n",
    "    def action_three(_):\n",
    "        return x/y\n",
    "\n",
    "    def action_four(_):\n",
    "        return y/x\n",
    "\n",
    "    key = jax.lax.switch(action, [action_one,action_two,action_three,action_four], None)\n",
    "    x, y = magic(key) * key\n",
    "    return jnp.array([x, y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_min, env_max = -0.1, 1.0\n",
    "action_space_n = 4\n",
    "\n",
    "dtype = jnp.float32\n",
    "\n",
    "state_space = spaces.Box(\n",
    "    low=env_min, high=max(env_min, env_max), shape=(2,), dtype=dtype\n",
    ")\n",
    "\n",
    "\n",
    "state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "action_space = spaces.Discrete(action_space_n)\n",
    "\n",
    "initial_state = jnp.array([-1.0, 1.0], dtype=dtype)\n",
    "target_state = jnp.array([0, 0], dtype=dtype)\n",
    "\n",
    "\n",
    "# Create the environment by creating an EnvironmentConfig object and passing it to the create_environment function\n",
    "\n",
    "config = EnvironmentConfig(\n",
    "    state_space=state_space,\n",
    "    action_space=action_space,\n",
    "    initial_state=initial_state,\n",
    "    target_state=target_state,\n",
    "    reward_function=jit(example_reward_function),\n",
    "    transition_function=jit(example_transition_function),\n",
    ")\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import optax\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import os\n",
    "import random as rd\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "env = create_environment(config)\n",
    "state_size = len(state_space.shape)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "\n",
    "batch_size = 32  # increase by powers of 2\n",
    "key = random.PRNGKey(0)\n",
    "num_episodes = 100  # Number of episodes to simulate\n",
    "num_iterations = 200  # Number of steps per episode\n",
    "\n",
    "output_dir = 'results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import jax\n",
    "from jax import jit, random, vmap, numpy as jnp\n",
    "\n",
    "import optax\n",
    "\n",
    "import random as rd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module):\n",
    "    state_size: int\n",
    "    action_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        q_values = nn.Dense(self.action_size)(x)\n",
    "        return q_values\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=3000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        \n",
    "        self.key = jax.random.PRNGKey(0)\n",
    "        self.model = DQNModel(state_size, action_size)\n",
    "        self.params = self.model.init(self.key, jnp.ones((1, state_size)))\n",
    "        self.optimizer = optax.adam(self.learning_rate)\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=self.params, tx=self.optimizer)\n",
    "        \n",
    "        # cant believe this worked\n",
    "        self.model.apply = jit(self.model.apply)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return rd.randrange(self.action_size)\n",
    "        state = jnp.array(state)\n",
    "        q_values = self.model.apply(self.state.params, state)\n",
    "        return jnp.argmax(q_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "       \n",
    "        minibatch = rd.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = jnp.array([experience[0] for experience in minibatch], dtype=jnp.float32)\n",
    "        actions = jnp.array([experience[1] for experience in minibatch], dtype=jnp.int32)\n",
    "        rewards = jnp.array([experience[2] for experience in minibatch], dtype=jnp.float32)\n",
    "        next_states = jnp.array([experience[3] for experience in minibatch], dtype=jnp.float32)\n",
    "        dones = jnp.array([experience[4] for experience in minibatch], dtype=jnp.bool_)\n",
    "\n",
    "        @jax.jit\n",
    "        def compute_target_q_values(rewards, gamma, futures, dones):\n",
    "            return rewards + gamma * futures * (1 - dones) \n",
    "\n",
    "        futures = jnp.max(self.model.apply(self.state.params, next_states), axis=-1)\n",
    "                \n",
    "        gamma = self.gamma\n",
    "        target_q_values = compute_target_q_values(rewards, gamma, futures, dones)\n",
    "\n",
    "\n",
    "        def loss_fn(params, state, action, target):\n",
    "            q_values = self.model.apply(params, state)\n",
    "            q_value = q_values[action]\n",
    "            loss = jnp.mean((target - q_value) ** 2)\n",
    "            return loss\n",
    "        \n",
    "        grad_fn = jax.grad(loss_fn)\n",
    "        vmap_grad_fn = vmap(grad_fn, in_axes=(None, 0, 0, 0))\n",
    "\n",
    "        # grads = jax.grad(loss_fn)(states, actions, target_q_values)\n",
    "        grads = vmap_grad_fn(self.state.params, states, actions, target_q_values)\n",
    "        average_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "                     \n",
    "        self.state = self.state.apply_gradients(grads=average_grads)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=self.model.apply, params=jnp.load(name), tx=self.optimizer\n",
    "        )\n",
    "\n",
    "    def save(self, name):\n",
    "        jnp.save(name, self.state.params)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# agent = Agent(state_size=4, action_size=2)\n",
    "# agent.remember([1, 2, 3, 4], 1, 1, [1, 2, 3, 5], False)\n",
    "# agent.act([1, 2, 3, 4])\n",
    "# agent.replay(1)\n",
    "# agent.save('model_params.npy')\n",
    "# agent.load('model_params.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"episode: {episode} state: {state}, reward: {reward}, action: none done: {done}\")\n",
    "#episode_reward = 0  # Track total reward per episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    state, _, _ = env.reset()\n",
    "    episode_reward = 0  # Track total reward per episode\n",
    "\n",
    "    for time in range(num_iterations):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward  # Accumulate reward per step\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or time == 199:\n",
    "            # need better logging\n",
    "            # add time in 00h 00m 00s then episode number score and loss\n",
    "            print(\n",
    "                f\"episode: {episode}/{num_episodes} end_state: {state}, score: {episode_reward}, done: {done}\"\n",
    "            )\n",
    "            break\n",
    "    \n",
    "    # Replay the experience\n",
    "    if len(agent.memory) > 32:\n",
    "        agent.replay(32)\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = rd.sample(agent.memory, batch_size)\n",
    "states = jnp.array([experience[0] for experience in minibatch], dtype=jnp.float32)\n",
    "actions = jnp.array([experience[1] for experience in minibatch], dtype=jnp.int32)\n",
    "rewards = jnp.array([experience[2] for experience in minibatch], dtype=jnp.float32)\n",
    "next_states = jnp.array([experience[3] for experience in minibatch], dtype=jnp.float32)\n",
    "dones = jnp.array([experience[4] for experience in minibatch], dtype=jnp.bool_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the target Q-values using JIT compilation\n",
    "@jax.jit\n",
    "def compute_target_q_values(rewards, gamma, futures, dones):\n",
    "    return rewards + gamma * futures * (1 - dones) \n",
    "\n",
    "gamma = agent.gamma\n",
    "futures = jnp.max(agent.model.apply(agent.state.params, next_states), axis=-1)\n",
    "target_q_values = compute_target_q_values(rewards, gamma, futures, dones)\n",
    "type(target_q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(states, actions, targets, model, params):\n",
    "    \"\"\"\n",
    "    Compute the loss values in parallel using JAX.\n",
    "\n",
    "    :param states: Array of states\n",
    "    :param actions: Array of actions\n",
    "    :param targets: Array of target Q-values\n",
    "    :param model: A Flax model\n",
    "    :param params: Parameters of the Flax model\n",
    "    :return: Array of loss values\n",
    "    \"\"\"\n",
    "    # Vectorize the model function to apply it to all states\n",
    "    def model_fn(state):\n",
    "        return model.apply(params, state)\n",
    "    \n",
    "    vectorized_model = vmap(model_fn)\n",
    "    \n",
    "    # Compute the Q-values for all states\n",
    "    q_values = vectorized_model(states)\n",
    "    \n",
    "    # Select the Q-values corresponding to the actions taken\n",
    "    q_values = jnp.take_along_axis(q_values, actions[:, None], axis=1).squeeze()\n",
    "    \n",
    "    # Compute the squared differences between predicted Q-values and target Q-values\n",
    "    loss_values = (q_values - targets) ** 2\n",
    "    \n",
    "    return loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = compute_loss(states, actions, target_q_values, agent.model, agent.state.params)\n",
    "G = loss_fn_batch(states, actions, target_q_values, agent.model, agent.state.params)\n",
    "L == G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 3: Average all the gradients then just apply one\n",
    "# In this approach, we compute the gradients for all samples, average them, and then apply the averaged gradient.\n",
    "def loss_fn_batch(states, actions, targets, model, params):\n",
    "    def single_loss_fn(state, action, target):\n",
    "        q_values = model.apply(params, state)\n",
    "        q_value = q_values[action]\n",
    "        loss = (q_value - target) ** 2\n",
    "        return loss\n",
    "    vectorized_loss_fn = jax.vmap(single_loss_fn, in_axes=(0, 0, 0))\n",
    "    return vectorized_loss_fn(states, actions, targets)\n",
    "\n",
    "\n",
    "def update_step_average(states, actions, targets, model, train_state):\n",
    "    grads = jax.grad(loss_fn_batch)(train_state.params, states, actions, targets, model)\n",
    "    mean_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "    train_state = train_state.apply_gradients(grads=mean_grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "#train_state = update_step_average(states, actions, targets, model, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, state, action, target):\n",
    "    q_values = agent.model.apply(params, state)\n",
    "    q_value = q_values[action]\n",
    "    loss = jnp.mean((target - q_value) ** 2)\n",
    "    return loss\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "vmap_grad_fn = vmap(grad_fn, in_axes=(None, 0, 0, 0))\n",
    "\n",
    "sgrads = grad_fn(agent.state.params, states[8], actions[8], target_q_values[8])\n",
    "grads = vmap_grad_fn(agent.state.params, states, actions, target_q_values)\n",
    "average_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 2: Calculate all the gradients and then apply them all\n",
    "#In this approach, we compute the gradients for all samples and then apply them all at once.\n",
    " \n",
    "def loss_fn_batch(params, states, actions, targets, model):\n",
    "    def single_loss_fn(state, action, target):\n",
    "        q_values = model.apply(params, state)\n",
    "        q_value = q_values[action]\n",
    "        loss = (q_value - target) ** 2\n",
    "        return loss\n",
    "    vectorized_loss_fn = jax.vmap(single_loss_fn, in_axes=(None, 0, 0, 0))\n",
    "    return jnp.mean(vectorized_loss_fn(states, actions, targets))\n",
    "\n",
    "def update_step_batch(states, actions, targets, model, train_state):\n",
    "    grads = jax.grad(loss_fn_batch)(train_state.params, states, actions, targets, model)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "train_state = update_step_batch(states, actions, targets, model, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Get gradient, apply it, get the next one, apply it, etc.\n",
    "# In this approach, we compute and apply the gradient for each sample sequentially.\n",
    "\n",
    "import jax\n",
    "from flax.training import train_state\n",
    "\n",
    "def loss_fn(params, state, action, target, model):\n",
    "    q_values = model.apply(params, state)\n",
    "    q_value = q_values[action]\n",
    "    loss = (q_value - target) ** 2\n",
    "    return loss\n",
    "\n",
    "def update_step(state, action, target, model, train_state):\n",
    "    grads = jax.grad(loss_fn)(train_state.params, state, action, target, model)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "for state, action, target in zip(states, actions, targets):\n",
    "    train_state = update_step(state, action, target, model, train_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
