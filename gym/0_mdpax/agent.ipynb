{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gymnasium import spaces, Env\n",
    "from environment import EnvironmentConfig, create_environment, example_reward_function, example_transition_function\n",
    "from jax import jit, numpy as jnp\n",
    "env_width, env_height = 5, 5\n",
    "action_space_n = 4\n",
    "\n",
    "dtype = jnp.int32\n",
    "\n",
    "state_space = spaces.Box(\n",
    "    low=0, high=max(env_width, env_height), shape=(env_width, env_height), dtype=dtype\n",
    ")\n",
    "\n",
    "action_space = spaces.Discrete(action_space_n)\n",
    "\n",
    "initial_state = jnp.array([0, 0], dtype=dtype)\n",
    "target_state = jnp.array([4, 4], dtype=dtype)\n",
    "\n",
    "\n",
    "# Create the environment by creating an EnvironmentConfig object and passing it to the create_environment function\n",
    "\n",
    "config = EnvironmentConfig(\n",
    "    state_space=state_space,\n",
    "    action_space=action_space,\n",
    "    initial_state=initial_state,\n",
    "    target_state=target_state,\n",
    "    reward_function=jit(example_reward_function),\n",
    "    transition_function=jit(example_transition_function),\n",
    ")\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import optax\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import os\n",
    "import random as rd\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "env = create_environment(config)\n",
    "state_size = len(state_space.shape)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "\n",
    "batch_size = 32  # increase by powers of 2\n",
    "key = random.PRNGKey(0)\n",
    "num_episodes = 100  # Number of episodes to simulate\n",
    "num_iterations = 200  # Number of steps per episode\n",
    "\n",
    "output_dir = 'results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module):\n",
    "    state_size: int\n",
    "    action_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(24)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(24)(x)\n",
    "        x = nn.relu(x)\n",
    "        q_values = nn.Dense(self.action_size)(x)\n",
    "        return q_values\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.key = jax.random.PRNGKey(0)\n",
    "        self.model = DQNModel(state_size, action_size)\n",
    "        self.params = self.model.init(self.key, jnp.ones((1, state_size)))\n",
    "        self.optimizer = optax.adam(self.learning_rate)\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=self.params, tx=self.optimizer)\n",
    "        \n",
    "        self.model.apply = jit(self.model.apply)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return rd.randrange(self.action_size)\n",
    "        state = jnp.array(state)\n",
    "        q_values = self.model.apply(self.state.params, state)\n",
    "        return jnp.argmax(q_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "       \n",
    "        minibatch = rd.sample(self.memory, batch_size)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        states = jnp.array([experience[0] for experience in minibatch], dtype=jnp.float32)\n",
    "        actions = jnp.array([experience[1] for experience in minibatch], dtype=jnp.int32)\n",
    "        rewards = jnp.array([experience[2] for experience in minibatch], dtype=jnp.float32)\n",
    "        next_states = jnp.array([experience[3] for experience in minibatch], dtype=jnp.float32)\n",
    "        dones = jnp.array([experience[4] for experience in minibatch], dtype=jnp.bool_)\n",
    "\n",
    "        # Extract model parameters\n",
    "\n",
    "        # Compute the target Q-values using JIT compilation\n",
    "        @jax.jit\n",
    "        def compute_target_q_values(rewards, gamma, futures, dones):\n",
    "            return rewards + gamma * futures * (1 - dones) \n",
    "\n",
    "        gamma = self.gamma\n",
    "        futures = jnp.max(self.model.apply(self.state.params, next_states), axis=-1)\n",
    "        target_q_values = compute_target_q_values(rewards, gamma, futures, dones)\n",
    "\n",
    "\n",
    "    \n",
    "                      \n",
    "        def loss_fn(state, action, target, i):\n",
    "            q_values = self.model.apply(self.params, state[i])\n",
    "            q_value = q_values[action[i]]\n",
    "    \n",
    "        \n",
    "            loss = jnp.mean((target[i] - q_value) ** 2)\n",
    "            return loss\n",
    "        \n",
    "        for i in range(batch_size): \n",
    "            grads = jax.grad(loss_fn)(states, actions, target_q_values)\n",
    "            self.state = self.state.apply_gradients(grads=grads)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def load(self, name):\n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=self.model.apply, params=jnp.load(name), tx=self.optimizer\n",
    "        )\n",
    "\n",
    "    def save(self, name):\n",
    "        jnp.save(name, self.state.params)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# agent = Agent(state_size=4, action_size=2)\n",
    "# agent.remember([1, 2, 3, 4], 1, 1, [1, 2, 3, 5], False)\n",
    "# agent.act([1, 2, 3, 4])\n",
    "# agent.replay(1)\n",
    "# agent.save('model_params.npy')\n",
    "# agent.load('model_params.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"episode: {episode} state: {state}, reward: {reward}, action: none done: {done}\")\n",
    "#episode_reward = 0  # Track total reward per episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    state, _, _ = env.reset()\n",
    "    episode_reward = 0  # Track total reward per episode\n",
    "\n",
    "    for time in range(num_iterations):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward  # Accumulate reward per step\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or time == 199:\n",
    "            # need better logging\n",
    "            # add time in 00h 00m 00s then episode number score and loss\n",
    "            print(\n",
    "                f\"episode: {episode}/{num_episodes} end_state: {state}, score: {episode_reward}, done: {done}\"\n",
    "            )\n",
    "            break\n",
    "    \n",
    "    # Replay the experience\n",
    "    if len(agent.memory) > 32:\n",
    "        agent.replay(32)\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = rd.sample(agent.memory, batch_size)\n",
    "states = jnp.array([experience[0] for experience in minibatch], dtype=jnp.float32)\n",
    "actions = jnp.array([experience[1] for experience in minibatch], dtype=jnp.int32)\n",
    "rewards = jnp.array([experience[2] for experience in minibatch], dtype=jnp.float32)\n",
    "next_states = jnp.array([experience[3] for experience in minibatch], dtype=jnp.float32)\n",
    "dones = jnp.array([experience[4] for experience in minibatch], dtype=jnp.bool_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the target Q-values using JIT compilation\n",
    "@jax.jit\n",
    "def compute_target_q_values(rewards, gamma, futures, dones):\n",
    "    return rewards + gamma * futures * (1 - dones) \n",
    "\n",
    "gamma = agent.gamma\n",
    "futures = jnp.max(agent.model.apply(agent.state.params, next_states), axis=-1)\n",
    "target_q_values = compute_target_q_values(rewards, gamma, futures, dones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(states, actions, targets, model, params):\n",
    "    \"\"\"\n",
    "    Compute the loss values in parallel using JAX.\n",
    "\n",
    "    :param states: Array of states\n",
    "    :param actions: Array of actions\n",
    "    :param targets: Array of target Q-values\n",
    "    :param model: A Flax model\n",
    "    :param params: Parameters of the Flax model\n",
    "    :return: Array of loss values\n",
    "    \"\"\"\n",
    "    # Vectorize the model function to apply it to all states\n",
    "    def model_fn(state):\n",
    "        return model.apply(params, state)\n",
    "    \n",
    "    vectorized_model = vmap(model_fn)\n",
    "    \n",
    "    # Compute the Q-values for all states\n",
    "    q_values = vectorized_model(states)\n",
    "    \n",
    "    # Select the Q-values corresponding to the actions taken\n",
    "    q_values = jnp.take_along_axis(q_values, actions[:, None], axis=1).squeeze()\n",
    "    \n",
    "    # Compute the squared differences between predicted Q-values and target Q-values\n",
    "    loss_values = (q_values - targets) ** 2\n",
    "    \n",
    "    return loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = compute_loss(states, actions, target_q_values, agent.model, agent.state.params)\n",
    "G = loss_fn_batch(states, actions, target_q_values, agent.model, agent.state.params)\n",
    "L == G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 3: Average all the gradients then just apply one\n",
    "# In this approach, we compute the gradients for all samples, average them, and then apply the averaged gradient.\n",
    "def loss_fn_batch(states, actions, targets, model, params):\n",
    "    def single_loss_fn(state, action, target):\n",
    "        q_values = model.apply(params, state)\n",
    "        q_value = q_values[action]\n",
    "        loss = (q_value - target) ** 2\n",
    "        return loss\n",
    "    vectorized_loss_fn = jax.vmap(single_loss_fn, in_axes=(0, 0, 0))\n",
    "    return vectorized_loss_fn(states, actions, targets)\n",
    "\n",
    "\n",
    "def update_step_average(states, actions, targets, model, train_state):\n",
    "    grads = jax.grad(loss_fn_batch)(train_state.params, states, actions, targets, model)\n",
    "    mean_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "    train_state = train_state.apply_gradients(grads=mean_grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "#train_state = update_step_average(states, actions, targets, model, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([9.6354194e-02, 1.1718886e+00, 1.0000000e+00, 2.4346778e-01,\n",
       "       7.1581337e-03, 2.0420419e-01, 1.5995663e+00, 1.0119374e-01,\n",
       "       3.6279994e-01, 1.5875581e-01, 3.9331135e-01, 1.1718886e+00,\n",
       "       1.2216524e+01, 2.6676621e+00, 1.1748057e-02, 1.3233642e-01,\n",
       "       1.4999023e-02, 1.1402357e+00, 1.0934278e+00, 5.4440212e-02,\n",
       "       1.3233642e-01, 6.3885707e-01, 2.7814272e-01, 2.8563923e-01,\n",
       "       1.1718886e+00, 8.9648360e-01, 1.8498833e-01, 7.6378258e+01,\n",
       "       2.7184488e-02, 2.9198304e-01, 5.9655152e-04, 1.3233642e-01],      dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0., dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(state, action, target):\n",
    "    q_values = agent.model.apply(agent.params, state)\n",
    "    q_value = q_values[action]\n",
    "\n",
    "\n",
    "    loss = jnp.mean((target - q_value) ** 2)\n",
    "    return loss\n",
    "        \n",
    "h, g = jax.grad(loss_fn)(states[2], actions[2], target_q_values[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.15441827, -0.02721383], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 2: Calculate all the gradients and then apply them all\n",
    "#In this approach, we compute the gradients for all samples and then apply them all at once.\n",
    " \n",
    "def loss_fn_batch(params, states, actions, targets, model):\n",
    "    def single_loss_fn(state, action, target):\n",
    "        q_values = model.apply(params, state)\n",
    "        q_value = q_values[action]\n",
    "        loss = (q_value - target) ** 2\n",
    "        return loss\n",
    "    vectorized_loss_fn = jax.vmap(single_loss_fn, in_axes=(None, 0, 0, 0))\n",
    "    return jnp.mean(vectorized_loss_fn(states, actions, targets))\n",
    "\n",
    "def update_step_batch(states, actions, targets, model, train_state):\n",
    "    grads = jax.grad(loss_fn_batch)(train_state.params, states, actions, targets, model)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "train_state = update_step_batch(states, actions, targets, model, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Get gradient, apply it, get the next one, apply it, etc.\n",
    "# In this approach, we compute and apply the gradient for each sample sequentially.\n",
    "\n",
    "import jax\n",
    "from flax.training import train_state\n",
    "\n",
    "def loss_fn(params, state, action, target, model):\n",
    "    q_values = model.apply(params, state)\n",
    "    q_value = q_values[action]\n",
    "    loss = (q_value - target) ** 2\n",
    "    return loss\n",
    "\n",
    "def update_step(state, action, target, model, train_state):\n",
    "    grads = jax.grad(loss_fn)(train_state.params, state, action, target, model)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "for state, action, target in zip(states, actions, targets):\n",
    "    train_state = update_step(state, action, target, model, train_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
