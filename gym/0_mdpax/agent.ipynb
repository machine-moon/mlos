{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gymnasium import spaces, Env\n",
    "from environment import EnvironmentConfig, create_environment, example_reward_function, example_transition_function\n",
    "from jax import jit, numpy as jnp\n",
    "from jax import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-0.1, 1.0, (2,), float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_min, env_max = -0.1, 1.0\n",
    "action_space_n = 4\n",
    "\n",
    "dtype = jnp.float32\n",
    "\n",
    "state_space = spaces.Box(\n",
    "    low=env_min, high=max(env_min, env_max), shape=(2,), dtype=dtype\n",
    ")\n",
    "\n",
    "\n",
    "state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "action_space = spaces.Discrete(action_space_n)\n",
    "\n",
    "initial_state = jnp.array([-1.0, 1.0], dtype=dtype)\n",
    "target_state = jnp.array([0, 0], dtype=dtype)\n",
    "\n",
    "\n",
    "# Create the environment by creating an EnvironmentConfig object and passing it to the create_environment function\n",
    "\n",
    "config = EnvironmentConfig(\n",
    "    state_space=state_space,\n",
    "    action_space=action_space,\n",
    "    initial_state=initial_state,\n",
    "    target_state=target_state,\n",
    "    reward_function=jit(example_reward_function),\n",
    "    transition_function=jit(example_transition_function),\n",
    ")\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import optax\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import os\n",
    "import random as rd\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "env = create_environment(config)\n",
    "state_size = len(state_space.shape)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "\n",
    "batch_size = 32  # increase by powers of 2\n",
    "key = random.PRNGKey(0)\n",
    "num_episodes = 100  # Number of episodes to simulate\n",
    "num_iterations = 200  # Number of steps per episode\n",
    "\n",
    "output_dir = 'results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module):\n",
    "    state_size: int\n",
    "    action_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        q_values = nn.Dense(self.action_size)(x)\n",
    "        return q_values\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=3000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.key = jax.random.PRNGKey(0)\n",
    "        self.model = DQNModel(state_size, action_size)\n",
    "        self.params = self.model.init(self.key, jnp.ones((1, state_size)))\n",
    "        self.optimizer = optax.adam(self.learning_rate)\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=self.params, tx=self.optimizer)\n",
    "        \n",
    "        self.model.apply = jit(self.model.apply)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return rd.randrange(self.action_size)\n",
    "        state = jnp.array(state)\n",
    "        q_values = self.model.apply(self.state.params, state)\n",
    "        return jnp.argmax(q_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "       \n",
    "        minibatch = rd.sample(self.memory, batch_size)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        states = jnp.array([experience[0] for experience in minibatch], dtype=jnp.float32)\n",
    "        actions = jnp.array([experience[1] for experience in minibatch], dtype=jnp.int32)\n",
    "        rewards = jnp.array([experience[2] for experience in minibatch], dtype=jnp.float32)\n",
    "        next_states = jnp.array([experience[3] for experience in minibatch], dtype=jnp.float32)\n",
    "        dones = jnp.array([experience[4] for experience in minibatch], dtype=jnp.bool_)\n",
    "\n",
    "        # Extract model parameters\n",
    "\n",
    "        # Compute the target Q-values using JIT compilation\n",
    "        @jax.jit\n",
    "        def compute_target_q_values(rewards, gamma, futures, dones):\n",
    "            return rewards + gamma * futures * (1 - dones) \n",
    "\n",
    "        gamma = self.gamma\n",
    "        futures = jnp.max(self.model.apply(self.state.params, next_states), axis=-1)\n",
    "        target_q_values = compute_target_q_values(rewards, gamma, futures, dones)\n",
    "\n",
    "\n",
    "        def loss_fn(params, state, action, target):\n",
    "            q_values = self.model.apply(params, state)\n",
    "            q_value = q_values[action]\n",
    "            loss = jnp.mean((target - q_value) ** 2)\n",
    "            return loss\n",
    "        \n",
    "        grad_fn = jax.grad(loss_fn)\n",
    "        vmap_grad_fn = vmap(grad_fn, in_axes=(None, 0, 0, 0))\n",
    "\n",
    "        grads = vmap_grad_fn(self.state.params, states, actions, target_q_values)\n",
    "        average_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "                     \n",
    "        # grads = jax.grad(loss_fn)(states, actions, target_q_values)\n",
    "        self.state = self.state.apply_gradients(grads=average_grads)\n",
    "        \n",
    "       \n",
    "\n",
    "    def load(self, name):\n",
    "        self.state = train_state.TrainState.create(\n",
    "            apply_fn=self.model.apply, params=jnp.load(name), tx=self.optimizer\n",
    "        )\n",
    "\n",
    "    def save(self, name):\n",
    "        jnp.save(name, self.state.params)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# agent = Agent(state_size=4, action_size=2)\n",
    "# agent.remember([1, 2, 3, 4], 1, 1, [1, 2, 3, 5], False)\n",
    "# agent.act([1, 2, 3, 4])\n",
    "# agent.replay(1)\n",
    "# agent.save('model_params.npy')\n",
    "# agent.load('model_params.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"episode: {episode} state: {state}, reward: {reward}, action: none done: {done}\")\n",
    "#episode_reward = 0  # Track total reward per episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m      8\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m----> 9\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# Accumulate reward per step\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n",
      "File \u001b[0;32m/home/library/workspace/mlos/gym/0_mdpax/environment.py:79\u001b[0m, in \u001b[0;36mMDPEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes a step in the environment based on the action provided.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m        (next_state, reward, done, info).\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_reward(next_state)\n\u001b[1;32m     81\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_done(next_state)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/library/workspace/mlos/gym/0_mdpax/environment.py:51\u001b[0m, in \u001b[0;36mMDPEnvironment.__init__.<locals>.<lambda>\u001b[0;34m(state, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_function \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtransition_function\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# setups, overide the functions with jax.jit\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transition \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m state, action: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_reward \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m state: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_function(state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_state)\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_done \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(\u001b[38;5;28;01mlambda\u001b[39;00m state: jnp\u001b[38;5;241m.\u001b[39marray_equal(state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_state))\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/library/workspace/mlos/gym/0_mdpax/environment.py:144\u001b[0m, in \u001b[0;36mexample_transition_function\u001b[0;34m(state, action, state_space_shape)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_right\u001b[39m(_):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mmin(state_space_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, y \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswitch\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmove_up\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove_down\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove_right\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39marray([x, y])\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/library/workspace/mlos/gym/0_mdpax/environment.py:142\u001b[0m, in \u001b[0;36mexample_transition_function.<locals>.move_right\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_right\u001b[39m(_):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mmin(\u001b[43mstate_space_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, y \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    state, _, _ = env.reset()\n",
    "    episode_reward = 0  # Track total reward per episode\n",
    "\n",
    "    for time in range(num_iterations):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward  # Accumulate reward per step\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or time == 199:\n",
    "            # need better logging\n",
    "            # add time in 00h 00m 00s then episode number score and loss\n",
    "            print(\n",
    "                f\"episode: {episode}/{num_episodes} end_state: {state}, score: {episode_reward}, done: {done}\"\n",
    "            )\n",
    "            break\n",
    "    \n",
    "    # Replay the experience\n",
    "    if len(agent.memory) > 32:\n",
    "        agent.replay(32)\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = rd.sample(agent.memory, batch_size)\n",
    "states = jnp.array([experience[0] for experience in minibatch], dtype=jnp.float32)\n",
    "actions = jnp.array([experience[1] for experience in minibatch], dtype=jnp.int32)\n",
    "rewards = jnp.array([experience[2] for experience in minibatch], dtype=jnp.float32)\n",
    "next_states = jnp.array([experience[3] for experience in minibatch], dtype=jnp.float32)\n",
    "dones = jnp.array([experience[4] for experience in minibatch], dtype=jnp.bool_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the target Q-values using JIT compilation\n",
    "@jax.jit\n",
    "def compute_target_q_values(rewards, gamma, futures, dones):\n",
    "    return rewards + gamma * futures * (1 - dones) \n",
    "\n",
    "gamma = agent.gamma\n",
    "futures = jnp.max(agent.model.apply(agent.state.params, next_states), axis=-1)\n",
    "target_q_values = compute_target_q_values(rewards, gamma, futures, dones)\n",
    "type(target_q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(states, actions, targets, model, params):\n",
    "    \"\"\"\n",
    "    Compute the loss values in parallel using JAX.\n",
    "\n",
    "    :param states: Array of states\n",
    "    :param actions: Array of actions\n",
    "    :param targets: Array of target Q-values\n",
    "    :param model: A Flax model\n",
    "    :param params: Parameters of the Flax model\n",
    "    :return: Array of loss values\n",
    "    \"\"\"\n",
    "    # Vectorize the model function to apply it to all states\n",
    "    def model_fn(state):\n",
    "        return model.apply(params, state)\n",
    "    \n",
    "    vectorized_model = vmap(model_fn)\n",
    "    \n",
    "    # Compute the Q-values for all states\n",
    "    q_values = vectorized_model(states)\n",
    "    \n",
    "    # Select the Q-values corresponding to the actions taken\n",
    "    q_values = jnp.take_along_axis(q_values, actions[:, None], axis=1).squeeze()\n",
    "    \n",
    "    # Compute the squared differences between predicted Q-values and target Q-values\n",
    "    loss_values = (q_values - targets) ** 2\n",
    "    \n",
    "    return loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = compute_loss(states, actions, target_q_values, agent.model, agent.state.params)\n",
    "G = loss_fn_batch(states, actions, target_q_values, agent.model, agent.state.params)\n",
    "L == G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 3: Average all the gradients then just apply one\n",
    "# In this approach, we compute the gradients for all samples, average them, and then apply the averaged gradient.\n",
    "def loss_fn_batch(states, actions, targets, model, params):\n",
    "    def single_loss_fn(state, action, target):\n",
    "        q_values = model.apply(params, state)\n",
    "        q_value = q_values[action]\n",
    "        loss = (q_value - target) ** 2\n",
    "        return loss\n",
    "    vectorized_loss_fn = jax.vmap(single_loss_fn, in_axes=(0, 0, 0))\n",
    "    return vectorized_loss_fn(states, actions, targets)\n",
    "\n",
    "\n",
    "def update_step_average(states, actions, targets, model, train_state):\n",
    "    grads = jax.grad(loss_fn_batch)(train_state.params, states, actions, targets, model)\n",
    "    mean_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "    train_state = train_state.apply_gradients(grads=mean_grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "#train_state = update_step_average(states, actions, targets, model, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, state, action, target):\n",
    "    q_values = agent.model.apply(params, state)\n",
    "    q_value = q_values[action]\n",
    "    loss = jnp.mean((target - q_value) ** 2)\n",
    "    return loss\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "vmap_grad_fn = vmap(grad_fn, in_axes=(None, 0, 0, 0))\n",
    "\n",
    "sgrads = grad_fn(agent.state.params, states[8], actions[8], target_q_values[8])\n",
    "grads = vmap_grad_fn(agent.state.params, states, actions, target_q_values)\n",
    "average_grads = jax.tree_util.tree_map(lambda x: jnp.mean(x, axis=0), grads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 2: Calculate all the gradients and then apply them all\n",
    "#In this approach, we compute the gradients for all samples and then apply them all at once.\n",
    " \n",
    "def loss_fn_batch(params, states, actions, targets, model):\n",
    "    def single_loss_fn(state, action, target):\n",
    "        q_values = model.apply(params, state)\n",
    "        q_value = q_values[action]\n",
    "        loss = (q_value - target) ** 2\n",
    "        return loss\n",
    "    vectorized_loss_fn = jax.vmap(single_loss_fn, in_axes=(None, 0, 0, 0))\n",
    "    return jnp.mean(vectorized_loss_fn(states, actions, targets))\n",
    "\n",
    "def update_step_batch(states, actions, targets, model, train_state):\n",
    "    grads = jax.grad(loss_fn_batch)(train_state.params, states, actions, targets, model)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "train_state = update_step_batch(states, actions, targets, model, train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Get gradient, apply it, get the next one, apply it, etc.\n",
    "# In this approach, we compute and apply the gradient for each sample sequentially.\n",
    "\n",
    "import jax\n",
    "from flax.training import train_state\n",
    "\n",
    "def loss_fn(params, state, action, target, model):\n",
    "    q_values = model.apply(params, state)\n",
    "    q_value = q_values[action]\n",
    "    loss = (q_value - target) ** 2\n",
    "    return loss\n",
    "\n",
    "def update_step(state, action, target, model, train_state):\n",
    "    grads = jax.grad(loss_fn)(train_state.params, state, action, target, model)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n",
    "\n",
    "# Example usage\n",
    "for state, action, target in zip(states, actions, targets):\n",
    "    train_state = update_step(state, action, target, model, train_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
