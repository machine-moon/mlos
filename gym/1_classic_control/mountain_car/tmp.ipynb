{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: Imports\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65e26fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24eaaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PART 2: DQN Agent Defined\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.002\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model().cuda()\n",
    "       \n",
    "        self.loss_function = torch.nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.scaler = GradScaler()  # Initialize the GradScaler\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 24),\n",
    "            nn.GELU(),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.GELU(),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(24, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).cuda()\n",
    "        act_values = self.model(state)\n",
    "        return torch.argmax(act_values).item()\n",
    "        # return np.argmax(act_values.cpu().detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).cuda()\n",
    "            next_state = torch.FloatTensor(next_state).cuda()\n",
    "            action = torch.LongTensor([action]).cuda()\n",
    "            reward = torch.FloatTensor([reward]).cuda()\n",
    "            done = torch.FloatTensor([done]).cuda()\n",
    "\n",
    "\n",
    "          # Forward pass with autocast\n",
    "            with autocast():\n",
    "                q_values = self.model(state)\n",
    "                next_q_values = self.model(next_state)\n",
    "                q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "                next_q_value = next_q_values.max(1)[0]\n",
    "                expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "                loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "            # Scaled backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "        self.model.cuda()\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02ff6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PART 3: Create Gym Environment and Assign Vars\n",
    "env = gym.make('CartPole-v1',render_mode=\"human\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "batch_size = 1024\n",
    "\n",
    "output_dir = 'results/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3ce1f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: 11, e: 1.0\n",
      "episode: 1/1000, score: 8, e: 1.0\n",
      "episode: 2/1000, score: 31, e: 1.0\n",
      "episode: 3/1000, score: 17, e: 1.0\n",
      "episode: 4/1000, score: 10, e: 1.0\n",
      "episode: 5/1000, score: 15, e: 1.0\n",
      "episode: 6/1000, score: 31, e: 1.0\n",
      "episode: 7/1000, score: 31, e: 1.0\n",
      "episode: 8/1000, score: 20, e: 1.0\n",
      "episode: 9/1000, score: 16, e: 1.0\n",
      "episode: 10/1000, score: 13, e: 1.0\n",
      "episode: 11/1000, score: 18, e: 1.0\n",
      "episode: 12/1000, score: 14, e: 1.0\n",
      "episode: 13/1000, score: 14, e: 1.0\n",
      "episode: 14/1000, score: 24, e: 1.0\n",
      "episode: 15/1000, score: 8, e: 1.0\n",
      "episode: 16/1000, score: 21, e: 1.0\n",
      "episode: 17/1000, score: 53, e: 1.0\n",
      "episode: 18/1000, score: 24, e: 1.0\n",
      "episode: 19/1000, score: 11, e: 1.0\n",
      "episode: 20/1000, score: 26, e: 1.0\n",
      "episode: 21/1000, score: 19, e: 1.0\n",
      "episode: 22/1000, score: 21, e: 1.0\n",
      "episode: 23/1000, score: 23, e: 1.0\n",
      "episode: 24/1000, score: 14, e: 1.0\n",
      "episode: 25/1000, score: 13, e: 1.0\n",
      "episode: 26/1000, score: 23, e: 1.0\n",
      "episode: 27/1000, score: 61, e: 1.0\n",
      "episode: 28/1000, score: 10, e: 1.0\n",
      "episode: 29/1000, score: 9, e: 1.0\n",
      "episode: 30/1000, score: 21, e: 1.0\n",
      "episode: 31/1000, score: 20, e: 1.0\n",
      "episode: 32/1000, score: 35, e: 1.0\n",
      "episode: 33/1000, score: 25, e: 1.0\n",
      "episode: 34/1000, score: 23, e: 1.0\n",
      "episode: 35/1000, score: 17, e: 1.0\n",
      "episode: 36/1000, score: 41, e: 1.0\n",
      "episode: 37/1000, score: 10, e: 1.0\n",
      "episode: 38/1000, score: 55, e: 1.0\n",
      "episode: 39/1000, score: 26, e: 1.0\n",
      "episode: 40/1000, score: 33, e: 1.0\n",
      "episode: 41/1000, score: 10, e: 1.0\n",
      "episode: 42/1000, score: 17, e: 1.0\n",
      "episode: 43/1000, score: 14, e: 1.0\n",
      "episode: 44/1000, score: 25, e: 1.0\n",
      "episode: 45/1000, score: 17, e: 0.99\n",
      "episode: 46/1000, score: 74, e: 0.99\n",
      "episode: 47/1000, score: 11, e: 0.99\n",
      "episode: 48/1000, score: 12, e: 0.98\n",
      "episode: 49/1000, score: 13, e: 0.98\n",
      "episode: 50/1000, score: 39, e: 0.97\n",
      "episode: 51/1000, score: 20, e: 0.97\n",
      "episode: 52/1000, score: 12, e: 0.96\n",
      "episode: 53/1000, score: 15, e: 0.96\n",
      "episode: 54/1000, score: 17, e: 0.95\n",
      "episode: 55/1000, score: 21, e: 0.95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, e: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m     agent\u001b[38;5;241m.\u001b[39msave(output_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[36], line 81\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:453\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    451\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 453\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:350\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:350\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 4: Training Loop\n",
    "episodes = 1000\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state[0], [1, state[0].shape[0]])\n",
    "    state = torch.FloatTensor(state)\n",
    "    for time in range(500):  # set to a high number\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, next_state.shape[0]])\n",
    "        reward = reward if not done else -10\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"episode: {e}/{episodes}, score: {time}, e: {agent.epsilon:.2}\")\n",
    "            break\n",
    "    agent.replay(batch_size)\n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + \"weights_\" + \"{:04d}\".format(e) + \".hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937669c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PART 5: Testing Agent\n",
    "agent.load(output_dir + \"weights_\" + \"{:04d}\".format(50) + \".hdf5\")\n",
    "for e in range(10):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = np.reshape(next_state, [1, state_size])\n",
    "        if done:\n",
    "            print(f\"Test Episode: {e+1}/10, Score: {time}\")\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
