{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-04T08:01:52.785885Z",
     "start_time": "2024-08-04T08:01:52.780532Z"
    }
   },
   "source": [
    "from jax import jit, random, vmap, numpy as jnp, device_put\n",
    "from flax import linen as nn\n",
    "\n",
    "from abc import ABC, abstractmethod\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T07:57:58.136280Z",
     "start_time": "2024-08-04T07:57:58.117797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MuZeroNetwork:\n",
    "    def __new__(cls, config):\n",
    "        if config.network == \"fullyconnected\":\n",
    "            return MuZeroFullyConnectedNetwork(\n",
    "                config.observation_shape,\n",
    "                config.stacked_observations,\n",
    "                len(config.action_space),\n",
    "                config.encoding_size,\n",
    "                config.fc_reward_layers,\n",
    "                config.fc_value_layers,\n",
    "                config.fc_policy_layers,\n",
    "                config.fc_representation_layers,\n",
    "                config.fc_dynamics_layers,\n",
    "                config.support_size,\n",
    "            )\n",
    "        elif config.network == \"resnet\":\n",
    "            return MuZeroResidualNetwork(\n",
    "                config.observation_shape,\n",
    "                config.stacked_observations,\n",
    "                len(config.action_space),\n",
    "                config.blocks,\n",
    "                config.channels,\n",
    "                config.reduced_channels_reward,\n",
    "                config.reduced_channels_value,\n",
    "                config.reduced_channels_policy,\n",
    "                config.resnet_fc_reward_layers,\n",
    "                config.resnet_fc_value_layers,\n",
    "                config.resnet_fc_policy_layers,\n",
    "                config.support_size,\n",
    "                config.downsample,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'The network parameter should be \"fullyconnected\" or \"resnet\".'\n",
    "            )\n"
   ],
   "id": "e4952a2c9a5abf48",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T07:58:00.644537Z",
     "start_time": "2024-08-04T07:58:00.633433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def dict_to_device(dictionary, device):\n",
    "    device_dict = {}\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, jnp.ndarray):\n",
    "            device_dict[key] = device_put(value, device)\n",
    "        elif isinstance(value, dict):\n",
    "            device_dict[key] = dict_to_device(value, device)\n",
    "        else:\n",
    "            device_dict[key] = value\n",
    "    return device_dict"
   ],
   "id": "b92fe55c3992de57",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T08:05:04.152157Z",
     "start_time": "2024-08-04T08:05:04.137933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class AbstractNetwork(ABC, nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def initial_inference(self, observation):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def recurrent_inference(self, encoded_state, action):\n",
    "        pass\n",
    "\n",
    "    def get_weights(self):\n",
    "        return dict_to_cpu(self.state_dict())\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.load_state_dict(weights)\n",
    "\n"
   ],
   "id": "eb945d76864b9d98",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e690010c32729ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "##################################\n",
    "######## Fully Connected #########\n",
    "\n",
    "class MuZeroFullyConnectedNetwork(AbstractNetwork):\n",
    "    def __init__(\n",
    "            self,\n",
    "            observation_shape,\n",
    "            stacked_observations,\n",
    "            action_space_size,\n",
    "            encoding_size,\n",
    "            fc_reward_layers,\n",
    "            fc_value_layers,\n",
    "            fc_policy_layers,\n",
    "            fc_representation_layers,\n",
    "            fc_dynamics_layers,\n",
    "            support_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.action_space_size = action_space_size\n",
    "        self.full_support_size = 2 * support_size + 1\n",
    "\n",
    "        self.representation_network = torch.nn.DataParallel(\n",
    "            mlp(\n",
    "                observation_shape[0]\n",
    "                * observation_shape[1]\n",
    "                * observation_shape[2]\n",
    "                * (stacked_observations + 1)\n",
    "                + stacked_observations * observation_shape[1] * observation_shape[2],\n",
    "                fc_representation_layers,\n",
    "                encoding_size,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.dynamics_encoded_state_network = torch.nn.DataParallel(\n",
    "            mlp(\n",
    "                encoding_size + self.action_space_size,\n",
    "                fc_dynamics_layers,\n",
    "                encoding_size,\n",
    "            )\n",
    "        )\n",
    "        self.dynamics_reward_network = torch.nn.DataParallel(\n",
    "            mlp(encoding_size, fc_reward_layers, self.full_support_size)\n",
    "        )\n",
    "\n",
    "        self.prediction_policy_network = torch.nn.DataParallel(\n",
    "            mlp(encoding_size, fc_policy_layers, self.action_space_size)\n",
    "        )\n",
    "        self.prediction_value_network = torch.nn.DataParallel(\n",
    "            mlp(encoding_size, fc_value_layers, self.full_support_size)\n",
    "        )\n",
    "\n",
    "    def prediction(self, encoded_state):\n",
    "        policy_logits = self.prediction_policy_network(encoded_state)\n",
    "        value = self.prediction_value_network(encoded_state)\n",
    "        return policy_logits, value\n",
    "\n",
    "    def representation(self, observation):\n",
    "        encoded_state = self.representation_network(\n",
    "            observation.view(observation.shape[0], -1)\n",
    "        )\n",
    "        # Scale encoded state between [0, 1] (See appendix paper Training)\n",
    "        min_encoded_state = encoded_state.min(1, keepdim=True)[0]\n",
    "        max_encoded_state = encoded_state.max(1, keepdim=True)[0]\n",
    "        scale_encoded_state = max_encoded_state - min_encoded_state\n",
    "        scale_encoded_state[scale_encoded_state < 1e-5] += 1e-5\n",
    "        encoded_state_normalized = (\n",
    "                                           encoded_state - min_encoded_state\n",
    "                                   ) / scale_encoded_state\n",
    "        return encoded_state_normalized\n",
    "\n",
    "    def dynamics(self, encoded_state, action):\n",
    "        # Stack encoded_state with a game specific one hot encoded action (See paper appendix Network Architecture)\n",
    "        action_one_hot = (\n",
    "            torch.zeros((action.shape[0], self.action_space_size))\n",
    "            .to(action.device)\n",
    "            .float()\n",
    "        )\n",
    "        action_one_hot.scatter_(1, action.long(), 1.0)\n",
    "        x = torch.cat((encoded_state, action_one_hot), dim=1)\n",
    "\n",
    "        next_encoded_state = self.dynamics_encoded_state_network(x)\n",
    "\n",
    "        reward = self.dynamics_reward_network(next_encoded_state)\n",
    "\n",
    "        # Scale encoded state between [0, 1] (See paper appendix Training)\n",
    "        min_next_encoded_state = next_encoded_state.min(1, keepdim=True)[0]\n",
    "        max_next_encoded_state = next_encoded_state.max(1, keepdim=True)[0]\n",
    "        scale_next_encoded_state = max_next_encoded_state - min_next_encoded_state\n",
    "        scale_next_encoded_state[scale_next_encoded_state < 1e-5] += 1e-5\n",
    "        next_encoded_state_normalized = (\n",
    "                                                next_encoded_state - min_next_encoded_state\n",
    "                                        ) / scale_next_encoded_state\n",
    "\n",
    "        return next_encoded_state_normalized, reward\n",
    "\n",
    "    def initial_inference(self, observation):\n",
    "        encoded_state = self.representation(observation)\n",
    "        policy_logits, value = self.prediction(encoded_state)\n",
    "        # reward equal to 0 for consistency\n",
    "        reward = torch.log(\n",
    "            (\n",
    "                torch.zeros(1, self.full_support_size)\n",
    "                .scatter(1, torch.tensor([[self.full_support_size // 2]]).long(), 1.0)\n",
    "                .repeat(len(observation), 1)\n",
    "                .to(observation.device)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            value,\n",
    "            reward,\n",
    "            policy_logits,\n",
    "            encoded_state,\n",
    "        )\n",
    "\n",
    "    def recurrent_inference(self, encoded_state, action):\n",
    "        next_encoded_state, reward = self.dynamics(encoded_state, action)\n",
    "        policy_logits, value = self.prediction(next_encoded_state)\n",
    "        return value, reward, policy_logits, next_encoded_state\n",
    "\n",
    "###### End Fully Connected #######\n",
    "##################################"
   ],
   "id": "687fbf2b4ebac168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bec8bd5b776c7553"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
